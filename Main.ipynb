{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae69408-7f0b-49d6-afec-7fd7169a9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from typing import Dict, Union, Tuple\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b7942e-bfed-46ad-b6bf-c5489a6ddcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_dataset(\n",
    "        data_path: int, \n",
    "        bpe,\n",
    "        split: float, \n",
    "        block_size: int\n",
    "    ) -> Tuple[torch.utils.data.Dataset, torch.utils.data.Dataset]:\n",
    "\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    dataset_len = len(text)\n",
    "    train_size = int(dataset_len * split)\n",
    "\n",
    "    train_text = text[:train_size]\n",
    "    test_text = text[train_size:]\n",
    "    \n",
    "    train_set = OPDataset(train_text, bpe, block_size, train=True)\n",
    "    test_set = OPDataset(test_text, bpe, block_size, train=False)\n",
    "    return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd73e5e9-96fa-438f-9e24-8624cdc77f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_heads: int, \n",
    "            n_embed: int, \n",
    "            block_size: int\n",
    "        ):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        hidden_dim = n_embed // num_heads\n",
    "        self.mhsa = MultiHeadSelfAttention(num_heads, hidden_dim, n_embed, block_size)\n",
    "        self.feed_forward = FeedForward(n_embed)\n",
    "        self.norm1 = nn.LayerNorm(n_embed)\n",
    "        self.norm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.mhsa(self.norm1(x))\n",
    "        x = x + self.feed_forward(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            n_embed: int, \n",
    "            extend_width: int=4, \n",
    "            dropout: float=0.2\n",
    "        ):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(n_embed, extend_width*n_embed), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(extend_width*n_embed, n_embed), \n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            num_heads: int, \n",
    "            hidden_dim: int, \n",
    "            n_embed: int, \n",
    "            block_size: int, \n",
    "            dropout: float=0.2\n",
    "        ):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.heads = nn.ModuleList([SingleHead(hidden_dim, n_embed, block_size) for _ in range(self.num_heads)])\n",
    "        self.project = nn.Linear(n_embed, n_embed)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = torch.cat([sh(x) for sh in self.heads], dim=-1)\n",
    "        out = self.project(out)\n",
    "        out = self.drop(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SingleHead(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            hidden_dim: int, \n",
    "            n_embed: int, \n",
    "            block_size: int, \n",
    "            dropout: float=0.2\n",
    "        ):\n",
    "        super(SingleHead, self).__init__()\n",
    "        self.key = nn.Linear(n_embed, hidden_dim, bias=False)\n",
    "        self.query = nn.Linear(n_embed, hidden_dim, bias=False)\n",
    "        self.value = nn.Linear(n_embed, hidden_dim, bias=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        weights = q @ k.transpose(-2, -1) * C**(-0.5)\n",
    "        masked_weights = weights.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))\n",
    "        masked_probs = F.softmax(masked_weights, dim=-1)\n",
    "        masked_probs = self.drop(masked_probs)\n",
    "        v = self.value(x)\n",
    "        out = masked_probs @ v\n",
    "        return out\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            vocab_size: int, \n",
    "            block_size: int, \n",
    "            n_embed: int, \n",
    "            num_heads: int, \n",
    "            n_layers: int\n",
    "        ):\n",
    "        super(GPT, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        self.embedding = nn.Embedding(vocab_size, n_embed)\n",
    "        self.positional_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(num_heads, n_embed, block_size) for _ in range(n_layers)],\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(n_embed)        \n",
    "        self.fc = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, T = x.shape\n",
    "        token_embeddings = self.embedding(x) # B, T -> B, T, N_EMB\n",
    "        positional_embedding = self.positional_embedding_table(torch.arange(T, device=x.device)) # T -> T, C\n",
    "        token_embeddings = token_embeddings + positional_embedding # B, T, C + T, C -> B, T, C\n",
    "        blocks_out = self.blocks(token_embeddings)\n",
    "        blocks_out = self.norm(blocks_out)\n",
    "        logits = self.fc(blocks_out) # B, T, N_EMB -> B, T, C\n",
    "        logits = logits.reshape(B*T, self.vocab_size)\n",
    "        return logits\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, max_tokens: int) -> torch.Tensor:\n",
    "        t = idx.shape[1]\n",
    "        for _ in range(max_tokens):\n",
    "            idx_cond = idx[:, -self.block_size:]\n",
    "            logits = self.forward(idx_cond)\n",
    "            logits = logits.reshape(1, t, -1)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "            if t < self.block_size:\n",
    "                t += 1\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d12bee76-ce61-4df1-bda5-21f50e5c1a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "        train_loader: torch.utils.data.DataLoader, \n",
    "        model: torch.nn.Module, \n",
    "        criterion: torch.nn.Module, \n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        scheduler: torch.optim.lr_scheduler,\n",
    "        device: str\n",
    "    ) -> Dict[str, Union[torch.tensor, float]]:\n",
    " \n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    losses = torch.zeros(len(train_loader))\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        X = sample[\"X\"].to(device)\n",
    "        y = sample[\"y\"].to(device)\n",
    "        text = sample[\"text\"]\n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y.view(-1,))\n",
    "        losses[i] = loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    time_elapsed = time.time() - start\n",
    "    train_info = {\"loss\": torch.mean(losses), \"time\": time_elapsed}\n",
    "    return train_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e518529-cef5-4b28-8b90-e748fad26a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_one_epoch(\n",
    "        test_loader: torch.utils.data.DataLoader, \n",
    "        model: torch.nn.Module, \n",
    "        criterion: torch.nn.Module, \n",
    "        device: str\n",
    "    ) -> Dict[str, Union[torch.tensor, float]]:\n",
    "\n",
    "    start = time.time()\n",
    "    model.eval()\n",
    "    losses = torch.zeros(len(test_loader))\n",
    "    with torch.inference_mode():\n",
    "        for i, sample in enumerate(test_loader):\n",
    "            X = sample[\"X\"].to(device)\n",
    "            y = sample[\"y\"].to(device)\n",
    "            text = sample[\"text\"]\n",
    "            logits = model(X)\n",
    "            loss = criterion(logits, y.view(-1,))\n",
    "            losses[i] = loss.item()\n",
    "    time_elapsed = time.time() - start\n",
    "    test_info = {\"loss\": torch.mean(losses), \"time\": time_elapsed}\n",
    "    return test_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1fceeb7-e3c6-4c9e-9ae6-4c6f7cff8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "        model: torch.nn.Module, \n",
    "        device: str, \n",
    "        num_tokens: int\n",
    "    ):\n",
    "\n",
    "    idx = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "    print(train_set.decoder(model.generate(idx, num_tokens)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82b4ddfd-e82f-48b3-9423-b417118b942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            text: str, \n",
    "            bpe,\n",
    "            block_size: int, \n",
    "            train: bool=True\n",
    "        ):\n",
    "        super(OPDataset, self).__init__()\n",
    "        self.text = text\n",
    "\n",
    "        self.encoder = lambda s: bpe.encode(s)\n",
    "        self.decoder = lambda l: bpe.decode(l)\n",
    "        self.data = torch.tensor(self.encoder(self.text), dtype=torch.long)\n",
    "        self.block_size = block_size\n",
    "        self.train = train\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Union[torch.Tensor, str]]:\n",
    "        if self.train:\n",
    "            idx = torch.randint(len(self.data) - self.block_size, (1,)).item()\n",
    "        else:\n",
    "            idx = index\n",
    "            \n",
    "        X = self.data[idx:idx+self.block_size]\n",
    "        y = self.data[idx+1:idx+self.block_size+1]\n",
    "        text = self.text[idx:idx+self.block_size]\n",
    "        sample = {\"X\": X, \"y\": y, \"text\": text}\n",
    "        return sample\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self.train:\n",
    "            return 5000\n",
    "        return len(self.data) - self.block_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d098947d-5495-4938-bd98-74196c4c8e81",
   "metadata": {},
   "source": [
    "# BPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4655368d-ae82-411f-8037-62f9a86ae916",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BytePairEncoder:\n",
    "    def __init__(self, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocabulary = None\n",
    "        self.id2voc = None\n",
    "        self.merges = []\n",
    "\n",
    "    def getVocabSize(self):\n",
    "        return len(self.vocabulary)\n",
    "\n",
    "    def save(self, file_path):\n",
    "        data = {\n",
    "            \"vocab_size\": self.vocab_size,\n",
    "            \"vocabulary\": self.vocabulary,\n",
    "            \"id2voc\": self.id2voc,\n",
    "            \"merges\": self.merges\n",
    "        }\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    def load(self, file_path):\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.vocab_size = data[\"vocab_size\"]\n",
    "        self.vocabulary = data[\"vocabulary\"]\n",
    "        self.id2voc = data[\"id2voc\"]\n",
    "        self.merges = data.get(\"merges\", [])\n",
    "\n",
    "    def train(self, corpus):\n",
    "\n",
    "        if isinstance(corpus, list) and all(isinstance(item, str) for item in corpus):\n",
    "            buff = \"\"\n",
    "            for item in corpus:\n",
    "                buff += item + \" . \"\n",
    "            corpus = buff\n",
    "            print(corpus)\n",
    "            del buff\n",
    "        \n",
    "        chars = sorted(list(set(corpus)))\n",
    "        vocabulary = {}\n",
    "        id2voc = {}\n",
    "        \n",
    "        for idx, char in enumerate(chars):\n",
    "            vocabulary[char] = idx\n",
    "\n",
    "        for char, idx in vocabulary.items():\n",
    "            id2voc[idx] = char\n",
    "        \n",
    "        token_ids = [vocabulary[char] for char in corpus]\n",
    "        current_id_len = len(chars)\n",
    "\n",
    "        while current_id_len < self.vocab_size:\n",
    "            pairs = self.get_pairs(token_ids)\n",
    "            \n",
    "            if not pairs:\n",
    "                break  # birleştirilecek çift yoksa loop u durdur\n",
    "                \n",
    "            best_pair = max(pairs, key=lambda x: pairs[x]) # En yüksek frekanslı çifti al\n",
    "            a, b = best_pair\n",
    "            new_token = id2voc[a] + id2voc[b]\n",
    "            \n",
    "            vocabulary[new_token] = current_id_len\n",
    "            id2voc[current_id_len] = new_token\n",
    "            self.merges.append((best_pair, current_id_len))\n",
    "            \n",
    "            token_ids = self.merge_pair(token_ids, best_pair, current_id_len)\n",
    "            current_id_len += 1\n",
    "\n",
    "        self.vocabulary = vocabulary\n",
    "        self.id2voc = id2voc\n",
    "        return self.vocabulary, self.id2voc\n",
    "\n",
    "    def get_pairs(self, token_ids):\n",
    "        pairs = defaultdict(int) # başlangıç valueları 0 olan bir sözlük oluşturur\n",
    "        for i in range(len(token_ids)-1):\n",
    "            pair = (token_ids[i], token_ids[i+1])\n",
    "            pairs[pair] += 1\n",
    "        return pairs\n",
    "\n",
    "    def merge_pair(self, token_ids, pair, new_id):\n",
    "        new_token_ids = []\n",
    "        i = 0\n",
    "        while i < len(token_ids):\n",
    "            if i < len(token_ids)-1 and (token_ids[i], token_ids[i+1]) == pair:\n",
    "                new_token_ids.append(new_id)\n",
    "                i += 2\n",
    "            else:\n",
    "                new_token_ids.append(token_ids[i])\n",
    "                i += 1\n",
    "        return new_token_ids\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = list(text)\n",
    "        token_ids = []\n",
    "        for char in tokens:\n",
    "            token_ids.append(self.vocabulary.get(char, self.vocabulary.get(' ', 0))) # unk = 0 = ' '\n",
    "        \n",
    "        for (pair, new_id) in self.merges:\n",
    "            token_ids = self.merge_pair(token_ids, pair, new_id)\n",
    "        \n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        return ''.join([self.id2voc[idx] for idx in token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "041cf622-8706-4aa4-af58-e0fd158611c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pth = \"train.txt\"\n",
    "with open(pth, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "text = text[:10000]\n",
    "\n",
    "block_size = 256\n",
    "vocab_size = 1000\n",
    "n_embed = 384\n",
    "num_heads = 6\n",
    "n_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dad7beb-5658-4f14-aa0f-28e773f35ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "bpe = BytePairEncoder(vocab_size)\n",
    "bpe.train(text)\n",
    "print(bpe.getVocabSize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "33d9d1c4-ca97-4e6a-8852-c9b1604c6d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = OPDataset(text, bpe, block_size, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c969ecdd-eae3-458c-bab7-ee4f8736ff53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len 2901\n",
      "sample torch.Size([256])\n",
      "sample torch.Size([256])\n",
      "sample 256\n",
      "sample\n",
      " Benden selam söylen vefasız yare\n",
      "Gurbet benim olsun sıla kendine\n",
      "Çekilmedik derdimizi bölüşek\n",
      "Başlı ben alayım sıla kendine\n",
      "\n",
      "Dökek derdimizi ölçek böl\n"
     ]
    }
   ],
   "source": [
    "print(\"len\", len(dataset))\n",
    "print(\"sample\", dataset[0][\"X\"].shape)\n",
    "print(\"sample\", dataset[0][\"y\"].shape)\n",
    "print(\"sample\", len(dataset[0][\"text\"]))\n",
    "print(\"sample\\n\", dataset[0][\"text\"][:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "daf1a07b-8f39-4282-8eee-f2b216085921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1000])\n"
     ]
    }
   ],
   "source": [
    "model = GPT(bpe.getVocabSize(), block_size, n_embed, num_heads, n_layers)\n",
    "inp = torch.ones((1,block_size), dtype=torch.long)\n",
    "out = model(inp)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e05ed29-f84b-4bb1-b42c-219b27306487",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"./results/\"\n",
    "epochs = 50\n",
    "block_size = 100\n",
    "split = 0.9\n",
    "batch_size = 512\n",
    "initial_lr = 3e-4\n",
    "min_lr = 1e-4\n",
    "evaluate_every = 10\n",
    "n_embed = 256\n",
    "num_heads = 4\n",
    "n_layers = 4\n",
    "device_id = 0\n",
    "\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be11aa58-69e0-4c33-ad88-c4d15be39a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = return_dataset(\"train.txt\", bpe, 0.9, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e13149f8-27ed-4734-8a45-1bbde22cd4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda:{}'.format(device_id) if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36fd671a-05f2-478e-ab9f-c7198ba430d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(bpe.getVocabSize(), block_size, n_embed, num_heads, n_layers)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4840c980-9dff-42aa-9e45-db24efac3bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At epoch: 1, train loss: 4.01, in 3.72 seconds\n",
      "At epoch: 2, train loss: 3.97, in 3.47 seconds\n",
      "At epoch: 3, train loss: 3.97, in 3.44 seconds\n",
      "At epoch: 4, train loss: 3.95, in 3.46 seconds\n",
      "At epoch: 5, train loss: 3.93, in 3.47 seconds\n",
      "At epoch: 6, train loss: 3.93, in 3.48 seconds\n",
      "At epoch: 7, train loss: 3.92, in 3.50 seconds\n",
      "At epoch: 8, train loss: 3.91, in 3.51 seconds\n",
      "At epoch: 9, train loss: 3.91, in 3.54 seconds\n",
      "At epoch: 10, train loss: 3.89, in 3.54 seconds\n",
      "\n",
      "At epoch: 10, test loss: 4.05, in 11.43 seconds\n",
      "\n",
      "At epoch: 11, train loss: 3.89, in 3.43 seconds\n",
      "At epoch: 12, train loss: 3.88, in 3.56 seconds\n",
      "At epoch: 13, train loss: 3.87, in 3.58 seconds\n",
      "At epoch: 14, train loss: 3.86, in 3.57 seconds\n",
      "At epoch: 15, train loss: 3.85, in 3.58 seconds\n",
      "At epoch: 16, train loss: 3.84, in 3.60 seconds\n",
      "At epoch: 17, train loss: 3.83, in 3.59 seconds\n",
      "At epoch: 18, train loss: 3.83, in 3.56 seconds\n",
      "At epoch: 19, train loss: 3.82, in 3.57 seconds\n",
      "At epoch: 20, train loss: 3.80, in 3.58 seconds\n",
      "\n",
      "At epoch: 20, test loss: 3.99, in 11.32 seconds\n",
      "\n",
      "At epoch: 21, train loss: 3.81, in 3.41 seconds\n",
      "At epoch: 22, train loss: 3.80, in 3.57 seconds\n",
      "At epoch: 23, train loss: 3.78, in 3.60 seconds\n",
      "At epoch: 24, train loss: 3.78, in 3.57 seconds\n",
      "At epoch: 25, train loss: 3.76, in 3.58 seconds\n",
      "At epoch: 26, train loss: 3.76, in 3.60 seconds\n",
      "At epoch: 27, train loss: 3.77, in 3.59 seconds\n",
      "At epoch: 28, train loss: 3.74, in 3.57 seconds\n",
      "At epoch: 29, train loss: 3.73, in 3.58 seconds\n",
      "At epoch: 30, train loss: 3.72, in 3.59 seconds\n",
      "\n",
      "At epoch: 30, test loss: 3.93, in 11.39 seconds\n",
      "\n",
      "At epoch: 31, train loss: 3.73, in 3.42 seconds\n",
      "At epoch: 32, train loss: 3.72, in 3.56 seconds\n",
      "At epoch: 33, train loss: 3.71, in 3.58 seconds\n",
      "At epoch: 34, train loss: 3.70, in 3.55 seconds\n",
      "At epoch: 35, train loss: 3.70, in 3.60 seconds\n",
      "At epoch: 36, train loss: 3.68, in 3.56 seconds\n",
      "At epoch: 37, train loss: 3.69, in 3.57 seconds\n",
      "At epoch: 38, train loss: 3.68, in 3.59 seconds\n",
      "At epoch: 39, train loss: 3.67, in 3.60 seconds\n",
      "At epoch: 40, train loss: 3.67, in 3.58 seconds\n",
      "\n",
      "At epoch: 40, test loss: 3.89, in 11.41 seconds\n",
      "\n",
      "At epoch: 41, train loss: 3.66, in 3.43 seconds\n",
      "At epoch: 42, train loss: 3.64, in 3.61 seconds\n",
      "At epoch: 43, train loss: 3.65, in 3.60 seconds\n",
      "At epoch: 44, train loss: 3.64, in 3.59 seconds\n",
      "At epoch: 45, train loss: 3.64, in 3.60 seconds\n",
      "At epoch: 46, train loss: 3.64, in 3.60 seconds\n",
      "At epoch: 47, train loss: 3.63, in 3.58 seconds\n",
      "At epoch: 48, train loss: 3.61, in 3.58 seconds\n",
      "At epoch: 49, train loss: 3.62, in 3.61 seconds\n",
      "At epoch: 50, train loss: 3.61, in 3.60 seconds\n",
      "\n",
      "At epoch: 50, test loss: 3.85, in 11.43 seconds\n",
      "\n",
      "\n",
      "Ve öğren bir şeyi duyduğum sevdana birlenirdi yapağı kıldı dolun düslü\n",
      "\n",
      "yine vardı da yaplıklarını sormaklığı rahat devri dalı der\n",
      "Öllerinde \n",
      "Gelseni  ben onları\n",
      "\n",
      "Marislim dağlardı\n",
      "Davraklarını sokak sorulusunun eller varlığın ormanı na hiç olmak\n",
      "\n",
      "---\n",
      "\n",
      "Ve hasretlerini \n",
      "Düşüp edindeli günah istemi\n",
      "Yeytim bel sözcükler\n",
      "Bir bu bağlıyor çıplak eder\n",
      "\n",
      "Yüzük durdu isteyi\n",
      "\n",
      "---\n",
      "\n",
      "Taatt ile hem bağlıyı hayatım\n",
      "Her yumağar \n",
      "Yüzününde gel aynaymış bir ilk ebilmek içinde bir gün gecey yalvar davgın sana bir gece\n",
      "\n",
      "Aşk ağen ey gidesin kaldı kesiyor ister\n",
      "Çiçimler meydanda en bu şu vardı göşmer ve adarın  büyüv bakın eyliliyim Handım\n",
      "Senin yalar dedim 'devmişti değil\n",
      "Bütün geceler hüksün baba debimem için bnasıl duygularla udurman\n",
      "Başında bu oturadan ekmedin abil hükürgünü\n",
      "\n",
      "Sonra delikanlık payal oksız yalnızlığım\n",
      "yeni adamın sevgiliği bekliğim sana çekmediğinde sevgin\n",
      "butyatakları toprağın;\n",
      "Kimini hatın uzakladını.\n",
      "\n",
      "Aşik el bile'den al olanmış'nuaktım \n",
      "Kurşar gizdi\n",
      "Sen sonra dışarınızdı\n",
      "Ölüm ve kendine dönüceğim sene\n",
      "Vey ise bilmi sahsa \n",
      "Gözüm nakan'la \n",
      "Ön\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=initial_lr)\n",
    "    \n",
    "# lr scheduler\n",
    "lambda_func = lambda epoch: max(0.99 ** epoch, min_lr / initial_lr)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_func)\n",
    "\n",
    "# Training loop    \n",
    "best_val_loss = 1e5\n",
    "for e in range(epochs):\n",
    "    train_info = train_one_epoch(train_dataloader, model, criterion, optimizer, scheduler, device)\n",
    "    print(\"At epoch: {}, train loss: {:.2f}, in {:.2f} seconds\".format(e+1, train_info[\"loss\"], train_info[\"time\"]))\n",
    "    if (e+1) % evaluate_every == 0:\n",
    "        test_info = test_one_epoch(test_dataloader, model, criterion, device)\n",
    "        print(\"\\nAt epoch: {}, test loss: {:.2f}, in {:.2f} seconds\\n\".format(e+1, test_info[\"loss\"], test_info[\"time\"]))\n",
    "        # save checkpoint\n",
    "        if best_val_loss > test_info[\"loss\"]:\n",
    "            torch.save(model.state_dict(), checkpoint_dir + \"model_epoch_{}_loss_{:.2f}.pt\".format(e, test_info[\"loss\"]))\n",
    "            best_val_loss = test_info[\"loss\"]\n",
    "\n",
    "    # Generate some text\n",
    "generate_text(model, device, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a1e4d-86de-4b02-b599-eda9da950ea1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
